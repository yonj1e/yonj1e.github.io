---
title: zheap设计概述
date: 2018-09-25 
categories: 
  - [PostgreSQL - Internals]
tags: 
  - PostgreSQL
  - undo
  - heap
---



https://github.com/EnterpriseDB/zheap

src/backend/access/zheap/README

Zheap
=====

The main purpose of this README is to provide an overview of the current design of zheap, a new storage format for PostgreSQL. This project has three major objectives:

这个README的主要目的是提供当前zheap的设计概述，这是PostgreSQL的一种新存储格式。该项目有三个主要目标：

1. Provide better control over bloat. In the existing  heap, we always create a new version of tuple when it is updated. These new versions are later removed by vacuum or hot-pruning, but this only frees up space for reuse by future inserts or updates; nothing is returned to the operating system. A similar problem occurs for tuples that are deleted. zheap will prevent bloat (a) by allowing in-place updates in common cases and (b) by reusing space as soon as a transaction that has performed a delete or non-in-place-update has committed. In short, with this new storage, whenever possible, we’ll avoid creating bloat in the first place.
2. 对膨胀提供更好的控制。在现有的 heap 中，我们在更新时总是创建新版本的元组。这些新版本后来通过 vacuum 或 hot-pruning 删除，但这只会释放空间，以便将来插入或更新时重复使用；空间不会返回给操作系统。删除元组也会出现类似的问题。在常见情况下，zheap 将（a）通过允许就地更新来防止膨胀，并且（b）通过在执行删除或非就地更新的事务提交后立即重用空间。简而言之，有了这个新的存储，只要有可能，我们就会避免在第一时间造成膨胀。
3. Reduce write amplification both by avoiding rewrites of heap pages and by making it possible to do an update that touches indexed columns without updating every index.
4. 通过避免重写堆页，以及在不更新每个索引的情况下进行涉及索引列的更新，可以减少写放大。
5. Reduce the tuple size by (a) shrinking the  tuple header and (b) eliminating most alignment padding.
6. 通过（a）缩小元组头和（b）消除大多数对齐填充来减少元组大小。

In-place updates will be supported except when (a) the new tuple is larger than the old tuple and the increase in size makes it impossible to fit the larger tuple onto the same page or (b) some column is modified which is covered by an index that has not been modified to support “delete-marking”. We have not begun work on delete-marking support for indexes yet, but intend to support it at least for btree indexes.

除了以下情况：（a）新的元组大于旧的元组，并且增大的大小使得无法将较大的元组放在同一页上，（b）某些列被修改，但索引未被修改以支持 “删除标记 delete-marking”，否则将支持就地更新。我们还没有开始对索引的删除标记支持的工作，但打算至少对 btree 索引提供支持。

General idea of zheap with undo
--------------------------------
Each backend is attached to a separate undo log to which it writes undo records. Each undo record is identified by a 64-bit undo record pointer of which the first 24 bits are used for the log number and the remaining 40 bits are used for an offset within that undo log. Only one transaction at a time can write to any given undo log, so the undo records for any given transaction are always consecutive.

每个后端都附加到一个单独的 undo 日志中，并将 undo 记录写入其中。每个 undo 记录由一个64位 undo 记录指针标识，其中前24位用于日志号，其余40位用于该 undo 日志内的偏移量。一次只有一个事务可以写入任何给定的 undo 日志，因此任何给定事务的 undo 记录始终是连续的。

Each zheap page has fixed set of transaction slots each of which contains the transaction information (transaction id and epoch) and the latest undo record pointer for that transaction. As of now, we have four transaction slots per page, but this can be changed. Currently, this is a compile-time option;  we can decide later whether such an option is desirable in general for users. Each transaction slot occupies 16 bytes. We allow the transaction slots to be reused after the transaction is committed which allows us to operate without needing too many slots. We can allow slots to be reused after a transaction abort as well, once undo actions are complete. We have observed that smaller tables say having very few pages typically need more slots; for larger tables, four slots are enough. In our internal testing, we have found that 16 slots give a very good performance, but more tests are needed to identify the right number of slots. The one known problem with the fixed number of slots is that it can lead to deadlock, so we are planning to add  a mechanism to allow the array of transactions slots to be continued on a separate overflow page. We also need such a mechanism to support cases where a large number of transactions acquire SHARE or KEY SHARE locks on a single page. The overflow pages will be stored in the zheap itself, interleaved with regular pages. These overflow pages will be marked in such a way that sequential scans will ignore them. We will have a meta page in zheap from which all overflow pages will be tracked.

每个 zheap 页面都有固定的一组事务槽，每个槽都包含事务信息（事务id 和 epoch）以及该事务的最新 undo 记录指针。截至目前，我们每页有四个事务处理槽，但这可以更改。目前，这是一个编译时选项；我们可以稍后决定用户是否希望使用这样的选项。每个事务槽占用16个字节。我们允许在事务提交后重用事务槽，这允许我们在不需要太多插槽的情况下进行操作。一旦 undo 操作完成，我们也可以允许在事务中止后重用插槽。我们注意到，较小的表示页面很少，通常需要更多的槽；对于较大的表，四个插槽就足够了。在我们的内部测试中，我们发现16个插槽提供了非常好的性能，但是还需要更多的测试来确定合适的插槽数量。固定插槽数量的一个已知问题是它可能导致死锁，因此我们计划添加一种机制，允许事务插槽数组在另一个溢出页面上继续。我们还需要这样的机制来支持大量事务在单个页面上获取 SHARE 或 KEY SHARE 锁的情况。溢出页面将存储在 zheap 中，并与常规页面交织在一起。这些溢出页将以顺序扫描将忽略它们的方式进行标记。我们将在 zheap 中有一个元页面，将从该页面跟踪所有溢出页面。

Typically, each zheap operation that modifies a page needs to first allocate a transaction slot on that page and then prepare an undo record for the operation. Then, in a critical section, it must write the undo record, perform the operation on heap page, update the transaction slot in a page, and finally write a WAL record for the operation. What we write as part of undo record and WAL depends on the operation.

通常，zheap 修改页面的每个操作都需要首先在该页面上分配事务槽，然后为该操作准备 undo 记录。然后，在一个关键部分，它必须编写 undo 记录，在堆页面上执行操作，更新页面中的事务槽，最后为操作写入 WAL 记录。我们写的 undo 记录和 WAL 的内容取决于操作。

Insert: Apart from the generic info, we write the TID (block number and offset number) of the tuple in undo record to identify the record during undo replay. In WAL, we write the offset number and the tuple, plus some minimal information which will be needed to regenerate the undo record during replay.

插入：除了通用信息之外，我们在 undo 记录中写入元组的 TID（块号和偏移号）以在 undo 重放期间识别记录。在 WAL 中，我们编写偏移数和元组，以及在重放期间重新生成 undo 记录所需的一些最小信息。

Delete: We write the complete tuple in the undo record even though we could get away with just  writing the TID as we do for an insert operation. This allows us to reuse the space occupied by the deleted record as soon as the transaction that has performed the operation commits. In WAL, we need to write the tuple only if full page writes are not enabled. If full page writes are enabled, we can rely on the page state to be same during recovery as it is during the actual operation, so we can retrieve the tuple from page to copy it into the undo record.

删除：我们在 undo 记录中编写完整的元组，即使我们只是像编写插入操作一样编写 TID。这允许我们在执行操作的事务提交后立即重用已删除记录占用的空间。在 WAL 中，只有在未启用全页写时才需要编写元组。如果启用了全页写，我们可以依赖页面状态在恢复期间与实际操作期间相同，因此我们可以从页面检索元组以将其复制到 undo 记录中。

Update: For in-place updates, we have to write the old tuple in the undo log and the new tuple in the zheap. We could optimize and write the diff tuple instead of the complete tuple in undo, but as of now, we are writing the complete tuple. For non-in-place updates, we write the old tuple and the new TID in undo; essentially this is equivalent to DELETE+INSERT. As for DELETE, this allows space to be recycled as soon as the updating transaction commits. In the WAL, we write a copy of the old tuple only if full pages writes are off and we write diff tuple for the new tuple (irrespective of the value of full-page writes) as we do in a current heap. In the case where a non-in-place-update happens to insert new tuple on a separate page, we write two undo records, one for old page and another for the new page. One can imagine that writing one undo record would be sufficient as we generally reach to a new tuple from old tuple if required, but we want to maintain a separate undo chain for each page.

更新：对于就地更新，我们必须在 undo 日志中编写旧元组，在 zheap 中编写新元组。我们可以在 undo 中优化和编写 diff 元组而不是完整的元组，但截至目前，我们正在编写完整的元组。对于非就地更新，我们在 undo 中编写旧元组和新 TID；本质上这相当于 DELETE + INSERT。对于 DELETE，这允许在更新事务提交后立即回收空间。在 WAL 中，只有当全页写关闭并且我们为新元组编写 diff 元组（不管全页写的值）时，我们才会编写旧元组的副本，就像我们在目前 heap 中所做的那样。在非就地更新恰好在单独的页面上插入新元组的情况下，我们编写两个 undo 记录，一个用于旧页面，另一个用于新页面。可以想象，编写一个 undo 记录就足够了，因为我们通常会根据需要从旧元组到达新元组，但我们希望为每个页面维护一个单独的 undo 链。

Select .. For [Key] Share/Update

Tuple locking will work much like a DML operation: reserve a transaction slot, update the tuple header with the lock information, write UNDO and WAL for the operation. To detect conflicts, we sometimes need to traverse the undo chains of all the active transactions on a page. We will always mark the tuple with the strongest lock mode that might be present, just as is done in the current heap, so that we can cheaply detect whether there is a potential conflict. If there is, we must get information about all the locks from undo in order to decide whether there is an actual conflict. The tuple will always contain either the strongest locker information or if all the lockers are of same strength, then it will contain the latest locker information. Whenever there is more than one locker operating on a tuple, we set the multi-locker bit on a tuple to indicate that the tuple has multiple lockers. Note, that we clear the multi-locker bit lazily (which means when we decide to wait for all the lockers to go away and there is no more locker alive on the tuple). During Rollback operation, we retain the strongest locker information on the tuple if there are multiple lockers on a tuple. This is because the conflict detection mechanism works based on strongest locker. Now, even if we want to remove strongest locker information, we don't have second strongest locker information handy.

元组锁定将像 DML 操作一样工作：保留事务槽，使用锁信息更新元组头，为操作写 UNDO 和 WAL。为了检测冲突，我们有时需要遍历页面上所有活动事务的 undo 链。我们将始终使用可能存在的最强锁模式标记元组，就像在目前 heap 中一样，以便我们可以低成本地检测是否存在潜在冲突。如果有，我们必须从 undo 中获取有关所有锁的信息，以便确定是否存在实际冲突。元组将始终包含最强锁信息，或者如果所有锁具有相同的强度，则它将包含最新的锁信息。每当在元组上运行多个锁时，我们在元组上设置多个锁位以指示元组具有多个锁。注意，我们懒惰地清除多个锁（这意味着当我们决定等待所有的锁消失并且元组上没有更多的锁存活时）。在回滚操作期间，如果元组上有多个锁，我们会在元组上保留最强的锁信息。这是因为冲突检测机制基于最强的锁工作。现在，即使我们想要删除最强的锁信息，我们也没有第二个最强的锁信息。

Copy: Similar to insert, we need to store the corresponding TID (block number, offset number) for a tuple in undo to identify the same during undo replay. But, we can minimize the number of undo records written for a page. First, we identify the unused offset ranges for a page, then insert one undo record for each offset range. For example, if we’re about to insert in offsets (2,3,5,9,10,11), we insert three undo records covering offset ranges (2,3), (5,5), and (9,11), respectively. For recovery, we insert a single WAL record containing the above-mentioned offset ranges along with some minimal information to regenerate the undo records and tuples.

复制：与插入类似，我们需要在 undo 重放期间为 undo 中的元组存储相应的 TID（块号，偏移号）以识别相同的内容。但是，我们可以最小化为页面编写的 undo 记录的数量。首先，我们确定页面的未使用偏移范围，然后为每个偏移范围插入一个 undo 记录。例如，如果我们要插入偏移量 (2,3,5,9,10,11)，我们分别插入三个 undo 记录，覆盖偏移范围 (2,3), (5,5) 和 (9,11)。为了恢复，我们插入一个包含上述偏移范围的 WAL 记录以及一些用来重新生成 undo 记录和元组的最小的信息。

Scans: During scans, we need to make a copy of the tuple instead of just holding the pin on a page. In the current heap, holding a pin on the buffer containing the tuple is sufficient because operations like vacuum which can rearrange the page always take a cleanup lock on a buffer. In zheap, however, in-place-updates work with just a exclusive lock on a buffer, so a tuple to which we hold a pointer might be updated under us.

扫描：在扫描期间，我们需要制作元组的副本，而不是仅仅将针固定在页面上。在目前 heap 中，在包含元组的缓冲区上保持一个引脚就足够了，因为像 vacuum 这样可以重新排列页面的操作总是会对缓冲区进行清理锁定。但是，在 zheap 中，就地更新仅使用缓冲区上的独占锁，因此我们持有指针的元组可能会在我们下面更新。

Insert .. On Conflict: The design is similar to current heap such that we use the speculative token to detect conflicts. We store the speculative token in undo instead of in the tuple header (CTID) simply because zheap’s tuple header doesn’t have CTID. Additionally, we set a bit in tuple header to indicate speculative insertion. ZheapTupleSatisfiesDirty routine checks this bit and fetches a speculative token from undo.

Insert .. On Conflict：设计类似于目前 heap ，因此我们使用推测标记来检测冲突。我们将推测令牌存储在 undo 中而不是存储在元组头（CTID）中，因为zheap的元组头没有CTID。另外，我们在元组头中设置一点来表示推测性插入。ZheapTupleSatisfiesDirty 例程检查此位并从 undo 中获取推测令牌。

Toast Tables: Toast tables can use zheap, too. Since zheap uses shorter tuple headers, this saves space. In the future, someone might want to support in-place updates for toast table data instead of doing delete+insert as we do today.

Toast Tables：Toast 表也可以使用 zheap。由于 zheap 使用较短的元组头，因此节省了空间。将来，有人可能希望支持 toast 表数据的就地更新，而不是像我们今天那样进行 delete + insert。

SQL Operations: All SQL operations that either need to interact with a heap (scans, ALTER TABLE, etc.) or require a HeapTuple (like joins, ORDER BY, ANALYZE, COPY, etc.) need to be changed to interact with zheap pages or zheap tuples. For now, we have taken the approach of  writing converter functions for tuples (i.e. zheap_to_heap, heap_to_zheap) to avoid changing the whole backend to accept to Zheap pages and tuples. Operations which need to access pages still need to be modified.  For all performance-critical operations, we operate directly on zheap pages and zheap tuples to avoid the cost of conversion. We think that some of this needs to be changed in response to whatever conclusions are reached regarding the proposed storage API.

SQL操作：需要更改与堆交互的所有 SQL 操作（scans，ALTER TABLE等）或需要 HeapTuple（如 joins，ORDER BY，ANALYZE，COPY 等）以与 zheap 页面交互或 zheap 元组。目前，我们采用了为元组编写转换器函数的方法（即 zheap_to_heap，heap_to_zheap），以避免将整个后端更改为接受 Zheap 页面和元组。仍需要修改需要访问页面的操作。对于所有性能关键型操作，我们直接在 zheap 页面和 zheap 元组上运行，以避免转换成本。我们认为其中一些需要根据有关建议的存储 API 的任何结论进行更改。

Transaction slot reuse
-----------------------
Transaction slots can be freely reused if the transaction is committed and all-visible, or if the transaction is aborted and undo actions for that transaction, at least relating to that page, have been performed. If the transaction is committed but not yet all-visible, we can reuse the slot after writing an additional, special undo record that lets us make subsequent tuple visibility decisions correctly.

如果事务被提交并且全部可见，或者如果事务被中止并且已经执行了针对该事务的 undo 动作（至少与该页面相关），则可以自由地重用事务槽。如果事务已提交但尚未全部可见，我们可以在编写其他特殊 undo 记录后重用该插槽，以便我们正确地进行后续元组可见性决策。

For committed transactions, there are two possibilities. If the transaction slot is not referenced by any tuple in the page, we simply clear the xid from the transaction slot. The undo record pointer is kept as it is to ensure that we don't break the undo chain for that slot. Otherwise, we write an undo record for each tuple that points to one of the committed transactions. We also mark the tuple indicating that the associated slot has been reused. In such a case, it is quite possible that the tuple has not been modified, but it is still pointing to transaction slot which has been reused for a new transaction which is not yet all-visible. During the visibility check for such a tuple, it might appear that the tuple is modified by a current transaction which is clearly wrong and can lead to wrong results.

对于已提交事务，有两种可能性。如果页面中的任何元组都没有引用事务槽，我们只需清除事务槽中的xid即可。 undo 记录指针保持不变，以确保我们不会破坏该插槽的撤消链。否则，我们为每个元组编写一个指向已提交事务之一的 undo 记录。我们还标记元组，指示相关的插槽已被重用。在这种情况下，很可能没有修改元组，但它仍然指向已被重用于尚未全部可见的新事务的事务槽。在对这样的元组进行可见性检查期间，可能看起来元组被当前事务修改，这显然是错误的并且可能导致错误的结果。

Subtransactions
----------------
zheap only uses the toplevel transaction ID; subtransactions that modify a zheap do not need separate transaction IDs. In the regular heap, when subtransactions are present, the subtransaction’s XID is used to make tuple visibility decisions correctly. In a zheap, subtransaction abort is instead handled by using undo to reverse changes to the zheap pages. This design minimizes consumption of transaction slots and pg_xact space, and ensures that all undo records for a toplevel transaction remain consecutive in the undo log.

zheap只使用toplevel事务ID; 修改zheap的子事务不需要单独的事务ID。在常规堆中，当存在子事务时，子事务的XID用于正确地进行元组可见性决策。在zheap中，通过使用撤消来反转对zheap页面的更改来处理子事务中止。此设计最大限度地减少了事务槽和pg_xact空间的消耗，并确保顶层事务的所有 undo 记录在 undo 日志中保持连续。

Reclaiming space within a page
-------------------------------
Space can be reclaimed within a page after (a) a delete, (b) a non-in-place update, or (c) an in-place update that reduces the width of the tuple. We can reuse the space when as soon as  the transaction that has performed the operation has committed. We can also reclaim space after inserts or non-in-place updates have been undone. There is some difference between the way space is reclaimed for transactions that are committed and all-visible vs. the transactions that are committed but still not all-visible. In the former case, we can just indicate in the line pointer that the corresponding item is dead whereas for later we need the capability to fetch the prior version of a tuple for transactions to which the delete is not visible. To allow that, we copy the transaction slot information into the line pointer so that we can easily reach the prior version of the tuple. As a net result, the space for a deleted tuple can be reclaimed immediately after the delete commits, but the space consumed by line pointer can only be freed once we delete the corresponding index tuples. For an aborted transaction, space can be reclaimed once undo is complete. We set the prune xid in page header during delete or update operations and during rollback of inserts to permit pruning to happen only when there is a possible benefit. When we try to prune, we first check if the prune xid is in progress; only if not will we attempt to prune the page.

在（a）删除，（b）非就地更新或（c）减少元组宽度的就地更新之后，可以在页面内回收空间。一旦执行了操作的事务提交，我们就可以重用该空间。我们还可以在插入或非就地更新撤消后回收空间。对于已提交和全部可见的事务，已回收空间的回收方式与已提交但仍未完全可见的事务之间存在一些差异。在前一种情况下，我们可以在行指针中指示相应的项目是死的，而为了以后我们需要能够获取删除不可见的事务的元组的先前版本。为此，我们将事务槽信息复制到行指针中，以便我们可以轻松地到达元组的先前版本。作为最终结果，删除提交后可以立即回收已删除元组的空间，但只有删除相应的索引元组后才能释放行指针占用的空间。对于中止的事务，一旦撤消完成，就可以回收空间。我们在删除或更新操作期间以及在插入回滚期间在页眉中设置prune xid，以便仅在有可能的好处时才进行修剪。当我们尝试修剪时，我们首先检查修剪xid是否正在进行中;只有否，我们才会尝试修剪页面。

Pruning will be attempted when update operation lands to a page where there is not enough space to accommodate a new tuple. We can also allow pruning to occur when we evict the page from shared buffers or read the page from disk as those are I/O intensive operations, so doing some CPU intensive operation doesn't cost much.

当更新操作落到没有足够空间容纳新元组的页面时，将尝试修剪。我们还可以允许在从共享缓冲区中逐出页面或从磁盘读取页面时进行修剪，因为这些是I / O密集型操作，因此执行一些CPU密集型操作不会花费太多。

With the above idea, it is quite possible that sometimes we try to prune the page when there is no immediate benefit of doing so. For example, even after pruning, the page might still not have enough space in the page to accommodate new tuple. One idea is to track the space at the transaction slot level, so
that we can know exactly how much space can be freed in page after pruning, but that will lead to increase in a space used by each transaction slot.

有了上述想法，很有可能有时我们会尝试修剪页面，而这样做并没有立竿见影的好处。例如，即使在修剪之后，页面可能仍然没有足够的空间来容纳新的元组。一个想法是跟踪事务槽级别的空间，以便我们可以确切地知道修剪后页面中可以释放多少空间，但这将导致每个事务槽使用的空间增加。

We can also reuse space if a transaction frees up space on the page (e.g. by delete) and then tries to use additional space (e.g. by a subsequent insert). We can’t in general reuse space freed up by a transaction until it commits, because if it aborts we’ll need that space during undo; but an insert or update could reuse space freed up by earlier operations in the same transaction, since all or none of them will roll back. This is a good optimization, but this needs some more thought.

如果事务释放页面上的空间（例如通过删除），然后尝试使用额外的空间（例如通过后续插入），我们也可以重用空间。在提交之前，我们通常不能重用由事务释放的空间，因为如果它中止，我们在撤消期间将需要该空间; 但是插入或更新可以重用同一事务中早期操作释放的空间，因为它们中的全部或全部都不会回滚。这是一个很好的优化，但这需要更多的思考。

Free Space Map
---------------
We can optimistically update the freespace map when we remove the tuples from a page in the hope that eventually most of the transactions will commit and space will be available. Additionally, we might want to update FSM during aborts when space-consuming actions like inserts are rolled back. When requesting free space, we would need to adjust things so that we continue the search from the previous block instead of repeatedly returning the same block.

当我们从页面中删除元组时，我们可以乐观地更新自由空间映射，希望最终大多数事务都能提交并且空间可用。此外，我们可能希望在中止时更新FSM，此时会回滚插入空间等操作。在请求空闲空间时，我们需要调整内容，以便继续从前一个块继续搜索，而不是重复返回相同的块。

I think updating it on every such operation can be costly, so we can perform it only after some threshold number, so later we might want to add a facility to track potentially available freespace and merge into the main data structure. We also want to make FSM crash-safe, since we can’t count on VACUUM to recover free space that we neglect to record.

我认为在每个这样的操作上更新它可能是昂贵的，所以我们只能在一些阈值数之后执行它，所以稍后我们可能想要添加一个工具来跟踪潜在可用的自由空间并合并到主数据结构中。我们还希望使FSM安全可靠，因为我们不能指望VACUUM恢复我们忽略记录的可用空间。

Page format
------------
zheap uses a standard page header,  stores transaction slots in the special space.

zheap使用标准页眉，在特殊空间中存储事务槽。

Tuple format
-------------
The tuple header is reduced from 24 bytes to 5 bytes (8 bytes with alignment): 2 bytes each for informask and infomask2, and one byte for t_hoff. I think we might be able to squeeze some space from t_infomask, but for now, I have kept it as two bytes. All transactional information is stored in undo, so fields that store such information are not needed here.

元组头从24个字节减少到5个字节（带对齐的8个字节）：每个字节用于informask和infomask2，一个字节用于t_hoff。我想我们可以从t_infomask中挤出一些空间，但是现在，我把它保留为两个字节。所有事务信息都存储在 undo 中，因此这里不需要存储此类信息的字段。

The idea is that we occupy somewhat more space at the page level, but save much more at tuple level, so we come out ahead overall.

我们的想法是，我们在页面级别占用更多空间，但在元组级别节省更多，因此我们总体上领先。

Alignment padding
------------------
The work isn’t all done yet, but we plan to omit all  alignment padding for pass-by-value types. Even in the current heap, we never point directly to such values, so the alignment padding doesn’t help much; it lets us fetch the value using a single instruction, but that is all. Pass-by-reference types will work as they do in the heap. Many pass-by-reference data types will be varlena data types (typlen = -1) with short varlena headers so no alignment padding will be introduced in that case anyway, but if we have varlenas with 4-byte headers or if we have fixed-length pass-by-reference types (e.g. interval, box) then we'll still end up with padding. We can't directly access unaligned values; instead, we need to use memcpy. We believe that the space savings will more than pay for the additional CPU costs.

这项工作还没有完成，但我们计划省略所有对齐填充类型的传递填充。即使在目前 heap 中，我们也从不直接指向这样的值，因此对齐填充没有多大帮助; 它让我们使用单个指令获取值，但这就是全部。传递引用类型将像在堆中一样工作。许多pass-by-reference数据类型将是具有短varlena头的varlena数据类型（typlen = -1），因此在这种情况下不会引入对齐填充，但是如果我们有varlenas具有4字节头或者我们已经修复 -length-by-reference类型（例如interval，box）然后我们仍然会用padding结束。我们无法直接访问未对齐的值; 相反，我们需要使用memcpy。我们相信节省的空间将超过额外的CPU成本。

We won't need any alignment padding between tuples as we always make a copy of the tuple to support in-place updates. Likewise, we don’t need alignment padding between the tuple header and the tuple data.

我们不需要元组之间的任何对齐填充，因为我们总是制作元组的副本以支持就地更新。同样，我们不需要在元组头和元组数据之间进行对齐填充。

Undo chain
-----------
Each undo record header contains the location of previous undo record pointer of the transaction that is performing the operation. For example, if transaction T1 has updated the tuple two times, the undo record for the last update will have a link for undo record of the previous update. Thus, the undo records for a particular page in a particular transaction form a single, linked chain.

每个 undo 记录头包含正在执行操作的事务的先前 undo 记录指针的位置。例如，如果事务T1已将元组更新两次，则上次更新的 undo 记录将具有用于上一次更新的 undo 记录的链接。因此，特定事务中特定页面的 undo 记录形成单个链接链。

Snapshots and visibility
-------------------------
Given a TID and a snapshot, there are three possibilities: (a) the tuple currently stored at the given TID; (b) some tuple previously stored at the given TID and subsequently written to the undo log might be visible; or (c) there might be nothing visible at all. To check the visibility of a tuple, we fetch the transaction slot number stored in the tuple header, and then get the transaction id and undo record pointer from transaction slot. Next, we check the current tuple’s visibility based on transaction id fetched from transaction slot and the last operation performed on the tuple. For example, if the last operation on tuple is a delete and the xid is visible to our snapshot, then we return NULL indicating no visible tuple. But if the xid that has last operated on tuple is not visible to the snapshot, then we use the undo record pointer to fetch the prior tuple from undo and similarly check its visibility. The only difference in checking the visibility for the undo tuple is that the xid that previously operated on undo tuple is present in the undo record, so we can use that instead of relying on the transaction slot. If the tuple from undo is also not visible, then we fetch the prior tuple from the undo chain. We need to traverse undo chains until we find a visible tuple or reach theinitially inserted tuple; if that is also not visible, we can return NULL.

给定TID和快照，有三种可能性：（a）当前存储在给定TID的元组; （b）先前存储在给定TID并随后写入 undo 日志的某些元组可能是可见的;或（c）根本没有任何可见的东西。为了检查元组的可见性，我们获取存储在元组头中的事务槽号，然后从事务槽中获取事务id和undo记录指针。接下来，我们根据从事务槽中获取的事务id和在元组上执行的最后一个操作来检查当前元组的可见性。例如，如果对元组的最后一个操作是删除并且xid对我们的快照可见，那么我们返回NULL表示没有可见的元组。但是如果最后一个对元组进行操作的xid对快照不可见，那么我们使用 undo 记录指针从撤消中获取前一个元组，并类似地检查其可见性。检查撤消元组的可见性的唯一区别是先前在撤消元组上操作的xid存在于 undo 记录中，因此我们可以使用它而不是依赖于事务槽。如果撤消的元组也不可见，那么我们从 undo 链中获取前一个元组。我们需要遍历 undo 链，直到我们找到一个可见的元组或到达初始插入的元组;如果那也不可见，我们可以返回NULL。

During visibility checking of a tuple in a zheap page or an undo chain, if we find that the tuple’s transaction slot has been reused, we retrieve the transaction information (xid and cid that has modified the tuple) of that tuple from undo.

在zheap页面或撤消链中的元组的可见性检查期间，如果我们发现元组的事务槽已被重用，我们从undo中检索该元组的事务信息（已修改元组的xid和cid）。

EvalPlanQual mechanism
-----------------------
This works in basically the same way as for the existing heap. The only special consideration is that the updated tuple could have the same TID as the original one if it was updated in place, so we might want to optimize such that we need not release the buffer lock and again refetch the tuple. However, at this stage, we are not sure if there is any big advantage in such an optimization.

这与现有堆的工作方式基本相同。唯一特别的考虑因素是更新后的元组可能与原始元组具有相同的TID（如果它已在适当位置更新），因此我们可能需要进行优化，以便我们不需要释放缓冲区锁并再次重新获取元组。但是，在这个阶段，我们不确定这种优化是否有任何大的优势。

64-bit transaction ids
-----------------------
Transaction slots in zheap pages store both the epoch and the XID; this eliminates the confusion between a use of a given XID in the current epoch and a use in some previous epoch, which means that we never need to freeze tuples. The difference between the oldest running XID and the newest XID is still limited to 2 billion because of the way that snapshots work. Moreover, the oldest XID that still has undo must have an XID age less than 2 billion: among other problems, this is currently the limit for how long commit status data can be retained, and it would be bad if we had undo data but didn’t know whether or not to apply the undo actions. Currently, this limitation is enforced by piggybacking on the existing wraparound machinery.

zheap页面中的事务槽存储了纪元和XID; 这消除了在当前时期中使用给定XID与在某个先前时期中使用之间的混淆，这意味着我们永远不需要冻结元组。由于快照的工作方式，最旧的XID和最新的XID之间的差异仍然限制在20亿。此外，仍然具有撤销功能的最旧的XID必须具有小于20亿的XID年龄：除了其他问题之外，这是目前提交状态数据可以保留多长时间的限制，如果我们有撤消数据但是没有 不知道是否应用 undo 操作。目前，这种限制是通过搭载现有的环绕式机械来实施的。

Indexing
---------
Current index AMs are not prepared to cope with multiple tuples at the same TID with different values stored in the index column. We plan to introduce special index AM support for in-place updates; when an index lacks such support, any modification to the value stored in a column covered by that index will prevent the use of in-place update. Additionally, indexes lacking such support will still require routine vacuuming, which we believe can be avoided when such support is present.

当前索引AM未准备好处理在同一TID处的多个元组，其中不同的值存储在索引列中。我们计划为就地更新引入特殊索引AM支持; 当索引缺少此类支持时，对该索引所涵盖的列中存储的值的任何修改都将阻止使用就地更新。此外，缺乏这种支持的索引仍然需要常规吸尘，我们认为当存在此类支持时可以避免这种吸尘。

The basic idea is that we need to delete-mark index entries when they might no longer be valid, either because of a delete or because of an update affecting the indexed column. An in-place update that does not modify the indexed column need not delete-mark the corresponding index entries. Note that an entry which is delete-marked might still be valid for some snapshots; once no relevant snapshots remain, we can remove the entry. In some cases, we may remove a delete-mark from an entry rather than removing the entry, either because the transaction which applied the delete-mark has rolled back, or because the indexed column was changed from value A to value B and then eventually back to value A. It is very desirable for performance reasons to have be able to distinguish from the index page whether or not the corresponding heap tuple is definitely all-visible, but the delete-marking approach is not quite sufficient for this purpose unless recently-inserted tuples are also delete-marked -- and that is undesirable, since the delete-markings would have to be cleared after the inserting transaction committed, which might end up dirtying many or all index pages. An alternative approach is to write undo for index insertions; then, the undo pointers in the index page tells us whether any index entries on that page may be recently-inserted, and the presence or absence of a delete-mark tells us whether any index entries on that page may no longer be valid. We intend to adopt this approach; it should allow index-only scans in most cases without the need for a separately-maintained visibility map.

基本思想是，当索引条目可能不再有效时，我们需要删除标记索引条目，原因可能是删除或因为影响索引列的更新。不修改索引列的就地更新不需要删除标记相应的索引条目。请注意，删除标记的条目可能仍然对某些快照有效;一旦没有相关的快照，我们就可以删除该条目。在某些情况下，我们可能会从条目中删除删除标记而不是删除条目，因为应用了删除标记的事务已回滚，或者因为索引列已从值A更改为值B，然后最终返回值A.出于性能原因，非常希望能够与索引页面区分对应的堆元组是否肯定是全部可见的，但删除标记方法对于此目的来说还不够，除非最近-inserted元组也是删除标记的 - 这是不可取的，因为删除标记必须在提交插入事务后清除，这可能最终弄脏许多或所有索引页。另一种方法是为索引插入编写undo;然后，索引页面中的撤消指针告诉我们该页面上的任何索引条目是否可能是最近插入的，并且删除标记的存在与否告诉我们该页面上的任何索引条目是否可能不再有效。我们打算采用这种方法;在大多数情况下，它应该允许仅索引扫描，而无需单独维护的可见性映射。

With this approach, an in-place update touches each index whose indexed columns are modified twice -- once to delete-mark the old entry (or entries) and once to insert the new entries. In some use cases, this will compare favorably with the existing approach, which touches every index exactly once. Specifically, it figures to reduce write amplification and index bloat when only one or a few indexed columns are updated at a time.

使用此方法，就地更新会触及索引列被修改两次的每个索引 - 一次删除 - 标记旧条目（或条目），一次插入新条目。在某些用例中，这将与现有方法相媲美，现有方法仅触及每个索引一次。具体而言，当一次仅更新一个或几个索引列时，它表示减少写入放大和索引膨胀。

Indexes that don't have delete-marking
---------------------------------------
Although indexes which lack delete-marking support still require vacuum, we can use undo to reduce the current three-pass approach to just two passes, avoiding the final heap scan. When a row is deleted, the vacuum will directly mark the line pointer as unused, writing an undo record as it does,  and then mark the corresponding index entries as dead. If vacuum fails midway through the undo can ensure that changes to the heap page are rolled back. If the vacuum goes on to commit, we don't need to revisit the heap page after index cleanup.

虽然缺少删除标记支持的索引仍然需要真空，但我们可以使用undo将当前的三遍方法减少到两遍，避免了最终的堆扫描。删除行时，vacuum将直接将行指针标记为未使用，写入 undo 记录，然后将相应的索引条目标记为已死。如果在撤消中途真空失败，则可以确保回滚对堆页面的更改。如果真空继续提交，我们不需要在索引清理后重新访问堆页面。

We must be careful about  TID reuse: we will only allow a TID to be reused when the transaction that has marked it as unused has committed. At that point, we can be assured that all the index entries corresponding to dead tuples will be marked as dead.

我们必须小心TID重用：我们只允许在标记为未使用的事务已提交时重用TID。此时，我们可以确保对应于死元组的所有索引条目都将被标记为已死。

Undo actions
-------------
We need to apply undo actions during explicit ROLLBACK or ROLLBACK TO SAVEPOINT operations and when an error causes a transaction or subtransaction abort. These actions reverse whatever work was done when the operation was performed; for example, if an update aborts, we must restore the old version of the tuple. During an explicit ROLLBACK or ROLLBACK TO SAVEPOINT, the transaction is in a good state and we have relevant locks on objects, so applying undo actions is straightforward, but the same is not true in error paths. In the case of a subtransaction abort, undo actions are performed after rolling back the subtransaction; the parent transaction is still good. In the case of a top-level abort, we begin an entirely new transaction to perform the undo actions. If this new transaction aborts, it can be retried later. For short transactions (say, one which generates only few kB of undo data), it is okay to apply the actions in the foreground but for longer transactions, it is advisable to delegate the work to an undo worker running in the background. The user is provided with a knob to control this behavior.

我们需要在显式ROLLBACK或ROLLBACK TO SAVEPOINT操作期间以及当错误导致事务或子事务中止时应用 undo 操作。这些操作可以逆转执行操作时所做的任何工作;例如，如果更新中止，我们必须恢复旧版本的元组。在显式ROLLBACK或ROLLBACK TO SAVEPOINT期间，事务处于良好状态，并且我们对对象具有相关锁定，因此应用 undo 操作很简单，但在错误路径中也不是这样。在子事务中止的情况下，在回滚子事务之后执行 undo 操作;父交易仍然很好。在顶级中止的情况下，我们开始一个全新的事务来执行 undo 操作。如果此新事务中止，则可以稍后重试。对于简短的事务（例如，只生成几KB的撤消数据的事务），可以在前台应用操作，但对于较长的事务，建议将工作委派给在后台运行的撤消工作程序。为用户提供旋钮以控制此行为。

Just like the DML operations to which they correspond, undo actions require us to write WAL. Otherwise, we would be unable to recover after a crash, and standby servers would not be properly updated.

就像它们对应的DML操作一样， undo 操作要求我们编写WAL。否则，我们将无法在崩溃后恢复，并且备用服务器将无法正确更新。

Applying undo actions
----------------------
In many cases, the same page will be modified multiple times by the same transaction. We can save locking and reduce WAL generation by collecting all of the undo records for a given page and then applying them all at once. However, it’s difficult to collect all of the records that might apply to a page from an arbitrarily large undo log in an efficient manner; in particular, we want to avoid rereading the same undo pages multiple times. Currently, we collect all consecutive records which apply to the same page and then apply them at one shot. This will cover the cases where most of the changes to heap pages are performed together. This algorithm could be improved. For example, we could do something like this:

在许多情况下，同一页面将由同一事务多次修改。我们可以通过收集给定页面的所有 undo 记录然后立即应用它们来保存锁定并减少WAL生成。但是，很难以有效的方式从任意大的撤销日志中收集可能适用于页面的所有记录; 特别是，我们希望避免多次重读相同的撤消页面。目前，我们收集适用于同一页面的所有连续记录，然后一次性应用它们。这将涵盖堆页面的大多数更改一起执行的情况。该算法可以改进。例如，我们可以这样做：

1. Read the last 32MB of undo for the transaction being undone (or all of the undo for the transaction, if there is less than 32MB).
2. 读取撤消的事务的最后32MB撤消（或者如果少于32MB，则读取事务的所有撤消）。
3. For each block that is touched by at least one record in the 32MB chunk, consolidate all records from this chunk that apply to that block.
4. 对于32MB块中至少一个记录触及的每个块，合并应用于该块的此块中的所有记录。
5. Sort the blocks by buffertag and apply the changes in ascending block-number order within each relation. Do this even for incomplete chains, so nothing is saved for later.
6. 通过buffertag对块进行排序，并在每个关系中以递增的块编号顺序应用更改。即使对于不完整的链也要这样做，因此以后不会保存任何内容。
7. Go to step 1.
8. 转到步骤1。

After applying undo actions for a page, we clear the transaction slot on a page if the oldest undo record we applied is the oldest undo record for that block generated by that transaction. Otherwise, we rewind the undo pointer in the page slot to the last record for that block that precedes the last undo record we applied. Because applying undo also always updates the transaction slot on the page, either rewinding it or clearlying it completely, we can always skip applying undo if we find that it’s already been applied previously. This could happen if the application of undo for a given transaction is interrupted a crash, or if it fails for some reason and is retried later.

在对页面应用 undo 操作后，如果我们应用的最旧 undo 记录是该事务生成的该块的最旧 undo 记录，则清除页面上的事务槽。否则，我们将页槽中的撤销指针回滚到该块的最后一条记录，该记录位于我们应用的最后一条 undo 记录之前。因为应用撤消也总是更新页面上的事务槽，无论是倒带还是完全清除它，如果我们发现它已经被先前应用，我们总是可以跳过应用撤消。如果给定事务的撤消应用程序因崩溃而中断，或者由于某种原因失败并且稍后重试，则可能发生这种情况。

This also prevents us from getting confused when the relation is (a) dropped, (b) rewritten using a new relfilenode, or (c) truncated to a shorter length (and perhaps subsequently re-extended). We apply the undo action only if the page contains the effect of the transaction for which we are applying undo actions, which can always be determined by examining the undo pointer in the transaction slot. If there is no transaction slot for the current transaction or if it is present but the undo record pointer in the slot is less than the undo record pointer of the undo record under consideration, the undo record can be ignored; it has already been applied or is no longer relevant. After a toplevel transaction abort, undo space is not recycled. However, after a subtransaction abort, we rewind the insert pointer to wherever it was at the start of the subtransaction, so that the undo for the toplevel transaction remains contiguous. We can’t do the same for toplevel aborts as that might contain special undo records related to transaction slots that were reused and we can’t afford to lose those. We write these special undo records only for toplevel transaction when it doesn’t find any free transaction slot or there is no transaction slot which contains transaction that is all-visible. In such cases, we reuse the committed transaction slots and write undo record which contains transaction information for them as we might need that information for transaction which still can’t see the committed transaction. We mark all such slots (that belongs to committed transactions) as available for reuse in one shot as doing it one slot at a time is quite costly. Since we might still need the special undo records for the transaction slots other than the current transaction, we can’t simply rewind the insert pointer. Note that we do this only for toplevel transactions; if we need the new slot when in a subtransaction, we reclaim only a single transaction slot.

当关系是（a）丢弃，（b）使用新的relfilenode重写，或（c）截断到较短的长度（并且可能随后重新扩展）时，这也可以防止我们混淆。仅当页面包含我们正在应用 undo 操作的事务的效果时，我们才应用 undo 操作，这总是可以通过检查事务槽中的撤消指针来确定。如果当前事务没有事务槽，或者如果它存在但是槽中的undo记录指针小于所考虑的undo记录的undo记录指针，则可以忽略undo记录;它已经被应用或不再相关。在顶层事务中止后，撤消空间不会被回收。但是，在子事务中止之后，我们将插入指针回滚到子事务开始处的任何位置，以便顶层事务的撤消保持连续。我们不能对顶级中止做同样的事情，因为它可能包含与重用的事务槽相关的特殊 undo 记录，我们不能丢失它们。当我们没有找到任何空闲事务槽或者没有包含全部可见事务的事务槽时，我们只为顶级事务编写这些特殊的 undo 记录。在这种情况下，我们重用已提交的事务槽并写入包含它们的事务信息的 undo 记录，因为我们可能需要那些仍无法看到已提交事务的事务信息。我们将所有这样的插槽（属于已提交的事务）标记为可以一次性重用，因为一次一个插槽是非常昂贵的。由于我们可能仍需要除当前事务之外的事务槽的特殊 undo 记录，因此我们不能简单地回滚插入指针。请注意，我们仅对顶级交易执行此操作;如果我们在子事务中需要新的插槽，我们只回收一个事务槽。

WAL consideration
------------------
Undo records are critical data and must be protected via WAL. Because an undo record must be written if and only if a page modification occurs, the undo record and the record for the page modification must be one and the same. Moreover, it is very important not to duplicate any information or store any unnecessary information, since WAL volume has a significant impact on overall system performance. In particular, there is no need to log the undo record pointer. We only need to ensure that after crash recovery undo record pointer is set correctly for each of the undo logs. To ensure that, we log a WAL record after XID change or at the first operation after checkpoint on undo log. The WAL record contains the information of insert point, log number, and Xid. This is enough to form an XID->(Log no. + Log insertion point) map which will be used to calculate the location of undo insertion during recovery.

 undo 记录是关键数据，必须通过WAL保护。因为当且仅当发生页面修改时必须写入 undo 记录，因此 undo 记录和页面修改的记录必须是同一个。此外，重要的是不要复制任何信息或存储任何不必要的信息，因为WAL卷对整体系统性能有重大影响。特别是，无需记录 undo 记录指针。我们只需要确保在崩溃恢复后为每个 undo 日志正确设置 undo 记录指针。为了确保这一点，我们在XID更改后或在 undo 日志的检查点之后的第一个操作时记录WAL记录。WAL记录包含插入点，日志编号和Xid的信息。这足以形成一个XID - >（日志编号+日志插入点）映射，该映射将用于计算恢复期间撤消插入的位置。

Another important consideration is that we don't need to have full page images for data in undo logs. Because the undo logs are always written serially, torn pages are not an issue. Suppose that  some block in one of the undo log is half filled and synced properly to disk; now, a checkpoint occurs  Next, we add some more data to the block. During the following checkpoint, the system crashes while flushing the block. The block could be in a condition such that first few bytes of it say 512 bytes are flushed appropriately and rest are old, but this won't cause problem because anyway old bytes will be intact and we can always start inserting new records at insert location in undo reconstructed during recovery.

另一个重要的考虑因素是我们不需要在 undo 日志中拥有数据的完整页面图像。由于 undo 日志始终是连续写入的，因此撕裂页面不是问题。假设其中一个撤销日志中的某些块被填半并正确同步到磁盘; 现在，检查点发生接下来，我们向块添加更多数据。在下一个检查点期间，系统在冲洗块时崩溃。该块可能处于这样的状态，即它的前几个字节表示512字节被适当地刷新并且休息是旧的，但这不会导致问题，因为无论如何旧的字节将是完整的并且我们总是可以开始在插入位置插入新记录 在恢复期间重建的撤消。

Undo Worker
------------
Currently, we have one background undo worker which performs undo actions as required and discards undo logs when they are no longer needed. Typically, it performs undo actions in response to a notification from a backend that has just aborted a transaction, but it will eventually detect and perform undo actions for any aborted transaction that does not otherwise get cleaned up.

目前，我们有一个后台撤消工作程序，它根据需要执行 undo 操作，并在不再需要时丢弃 undo 日志。通常，它执行 undo 操作以响应来自刚刚中止事务的后端的通知，但它最终将检测并执行任何未被清除的中止事务的 undo 操作。

We allow the undo worker to hibernate when there is no activity in the system. It hibernates for a minimum of 100ms and maximum of 10s, based on the time the system has remained idle. The undo worker mechanism will be extended to multiple undo workers to perform various jobs related to undo logs. For
example, if there are many pending rollback requests, then we can spawn a new undo worker which can help in processing the requests.

当系统中没有活动时，我们允许撤消工作程序休眠。根据系统闲置的时间，它至少休眠100毫秒，最多10秒。undo worker机制将扩展到多个undo工作站，以执行与undo日志相关的各种工作。例如，如果有许多挂起的回滚请求，那么我们可以生成一个新的撤消工作程序，它可以帮助处理请求。

UndoDiscard routine will be called by the undo worker for discarding the old undo records. UndoDiscard will process all the active undo logs.  It reads each undo log and checks whether the log corresponding to the first transaction in a log can be discarded (committed and all visible or aborted and undo already applied). If so, it moves to the next transaction in that undo log and continues in the same way. When it finds the first transaction whose undo  can't be discard yet, it first discards the undo log prior to that point and then remembers the transaction ID and undo location in shared memory. We consider undo for a transaction to be discardable once its XID  is smaller than oldestXmin.

撤消工作程序将调用UndoDiscard例程来丢弃旧的 undo 记录。UndoDiscard将处理所有活动的 undo 日志。它读取每个 undo 日志，并检查是否可以丢弃对应于日志中第一个事务的日志（已提交，所有可见或已中止，已撤消已应用）。如果是这样，它将移动到该 undo 日志中的下一个事务，并以相同的方式继续。当它找到尚无法丢弃撤消的第一个事务时，它首先丢弃该点之前的 undo 日志，然后记住共享内存中的事务ID和撤消位置。一旦XID小于oldestXmin，我们认为撤消事务是可废弃的。

Ideally, for the aborted transactions once the undo actions are replayed, we should be able to discard it’s undo, however, it might contain the undo records for reused transaction slots, so we can’t discard them until it becomes smaller than oldestXmin. Also, we can’t discard the undo for the aborted transaction if there is a preceding transaction which is committed and not all-visible. We can allow undo for aborted transactions to be discarded immediately if we remember in the first undo record of the transaction whether it contains undo of reused transaction slot. This will help the cases where the aborted transaction is the last transaction in undo log which is smaller than oldestXmin.

理想情况下，对于重放 undo 操作后中止的事务，我们应该能够丢弃它的撤销，但是，它可能包含重用事务槽的 undo 记录，因此我们不能丢弃它们，直到它变得小于oldestXmin。此外，如果存在提交且不是全部可见的先前事务，则我们不能丢弃对已中止事务的撤消。如果我们在事务的第一个 undo 记录中记住它是否包含重用事务槽的撤消，我们可以允许撤消中止事务的撤销。这将有助于中止事务是 undo 日志中最后一个事务的情况，该事务小于oldestXmin。

In Hot Standby mode, undo is discarded via WAL replay. Before discarding undo, we ensure that there are no queries running which need to get tuple from discarded undo. If there are any, a recovery conflict will occur, similar to what happens in other cases where a resource held by a particular backend prevents replay from advancing.

在热备模式下，通过WAL重放丢弃撤消。在丢弃撤销之前，我们确保没有运行的查询需要从丢弃的撤消中获取元组。如果存在，则会发生恢复冲突，类似于特定后端持有的资源阻止重放进行的其他情况。

For each undo log, the undo discard module maintains in memory array to hold the latest undiscarded xid and its start undo record pointer. The first XID in the undo log will be compared against GlobalXmin, if the xid is greater than GlobalXmin then nothing can be discarded;  otherwise, scan  the undo log starting with the oldest transaction it contains. To avoid processing every record in the undo log, we maintain a transaction start header in the first undo record written by any given transaction with space to store a pointer to the next transaction start undo record in that same undo log. This allows us to read an undo log transaction by transaction. When discarding undo, the background worker will read all active undo logs transaction by transaction until it finds a transaction with an XID greater than equal to the GlobalXmin. Once it finds such a transaction, it will discard all earlier undo records in that undo log, without even writing unflushed buffers to disk.

对于每个 undo 日志，撤消丢弃模块在内存阵列中维护以保存最新的未被丢弃的xid及其启动 undo 记录指针。 undo 日志中的第一个XID将与GlobalXmin进行比较，如果xid大于GlobalXmin，则不能丢弃任何内容; 否则，扫描 undo 日志，从它包含的最早的事务开始。为了避免处理撤销日志中的每个记录，我们在由任何给定事务写入的第一个 undo 记录中维护一个事务开始头，其中包含空间，以存储指向同一 undo 日志中下一个事务启动 undo 记录的指针。这允许我们按事务读取 undo 日志事务。丢弃撤消时，后台工作程序将按事务读取所有活动的 undo 日志事务，直到找到XID大于等于GlobalXmin的事务。一旦找到这样的事务，它就会丢弃该 undo 日志中的所有早期 undo 记录，甚至不会将未刷新的缓冲区写入磁盘。

Avoid fetching discarded undo record
-------------------------------------
The system must never attempt to fetch undo records which have already been discarded. Undo is generally discarded in the background by the undo worker, so we must account for the possibility that undo could be discarded at any time. We do maintain the oldest xid that have undo (oldestXidHavingUndo). Undo worker updates the value of oldestXidHavingUndo after discarding all the undo. Backends consider all transactions that precede oldestXidHavingUndo as all-visible, so they normally don’t try to fetch the undo which is already discarded. However, there is a race condition where backend decides that the transaction is greater than oldestXidHavingUndo and it needs to fetch the undo record and in the meantime undo worker discards the corresponding undo record. To handle such race conditions, we need to maintain some synchronization between backends and undo worker so that backends don’t try to access already discarded undo. So whenever undo fetch is trying to read a undo record from an undo log, first it needs to acquire a log->discard_lock in SHARED mode for the undo log and check that the undo record pointer is not less than log->oldest_data, if so, then don't fetch that undo record and return NULL (that means the previous version is all visible). And undo worker will take log->discard_lock in EXCLUSIVE mode for updating the log->oldest_data. We hold this lock just to update the value in shared memory, the actual discard happens outside this lock.

系统绝不能尝试获取已经丢弃的 undo 记录。撤消通常由撤消工作者在后台丢弃，因此我们必须考虑撤消可能随时丢弃的可能性。我们确实维护了具有撤销功能的最旧的xid（oldestXidHavingUndo）。在丢弃所有撤消后，撤消工作人员更新oldestXidHavingUndo的值。后端认为在最旧的XidHavingUndo之前的所有事务都是全可见的，因此它们通常不会尝试获取已经被丢弃的撤消。但是，存在竞争条件，其中后端决定事务大于oldestXidHavingUndo并且它需要获取 undo 记录，同时撤消工作者丢弃相应的 undo 记录。为了处理这种竞争条件，我们需要在后端和撤消工作者之间保持一些同步，以便后端不会尝试访问已经丢弃的撤销。因此，每当undo fetch尝试从 undo 日志中读取 undo 记录时，首先需要在SHARED模式下为 undo 日志获取log-> discard_lock，并检查 undo 记录指针是否不小于log-> oldest_data，如果所以，然后不要获取该 undo 记录并返回NULL（这意味着以前的版本都是可见的）。undo worker将在EXCLUSIVE模式下使用log-> discard_lock来更新log-> oldest_data。我们持有此锁仅用于更新共享内存中的值，实际丢弃发生在此锁之外。

Undo Log Storage
-----------------
This subsystem is responsible for lifecycle management of undo logs and backing files, associating undo logs with backends, allocating and managing space within undo logs. It provides access to undo log contents via shared buffers. The list of available undo logs is maintained in shared memory. Whenever a backend request for undo log allocation, it attaches a first free undo log to a backend, and if all existing undo logs are busy, it will create a new one. A set of APIs is provided by this subsystem to efficiently allocate
and discard undo logs.

该子系统负责撤销日志和后备文件的生命周期管理，将 undo 日志与后端相关联，在 undo 日志中分配和管理空间。它通过共享缓冲区提供对 undo 日志内容的访问。可用的 undo 日志列表保存在共享内存中。每当后端请求 undo 日志分配时，它会将第一个免费 undo 日志附加到后端，如果所有现有 undo 日志都忙，则会创建一个新日志。此子系统提供了一组API，以有效地分配和丢弃 undo 日志。

During a checkpoint, all the undo segment files and undo metadata files will be flushed to the disk.

在检查点期间，所有还原段文件和撤消元数据文件都将刷新到磁盘。